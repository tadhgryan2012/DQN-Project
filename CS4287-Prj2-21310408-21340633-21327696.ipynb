{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Project Part 3\n",
    "\n",
    "Code runs through until completion\n",
    "\n",
    "## Contributors: \n",
    "- Tadhg Ryan (21310408)\n",
    "- Craig Phayer (21340633)\n",
    "- Thomas McCarty (21327696)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (24.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting gymnasium==0.29.1 (from gymnasium[atari]==0.29.1)\n",
      "  Using cached gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from gymnasium==0.29.1->gymnasium[atari]==0.29.1) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from gymnasium==0.29.1->gymnasium[atari]==0.29.1) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\21310408\\appdata\\roaming\\python\\python310\\site-packages (from gymnasium==0.29.1->gymnasium[atari]==0.29.1) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from gymnasium==0.29.1->gymnasium[atari]==0.29.1) (0.0.4)\n",
      "Requirement already satisfied: shimmy<1.0,>=0.1.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]==0.29.1) (0.2.1)\n",
      "Requirement already satisfied: ale-py~=0.8.1 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]==0.29.1) (0.8.1)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]==0.29.1) (6.4.5)\n",
      "Using cached gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "Installing collected packages: gymnasium\n",
      "  Attempting uninstall: gymnasium\n",
      "    Found existing installation: gymnasium 1.0.0\n",
      "    Uninstalling gymnasium-1.0.0:\n",
      "      Successfully uninstalled gymnasium-1.0.0\n",
      "Successfully installed gymnasium-0.29.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gymnasium==0.29.1 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from gymnasium[ActionWrapper,accept-rom-license,box2d,classic_control]==0.29.1) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from gymnasium==0.29.1->gymnasium[ActionWrapper,accept-rom-license,box2d,classic_control]==0.29.1) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from gymnasium==0.29.1->gymnasium[ActionWrapper,accept-rom-license,box2d,classic_control]==0.29.1) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\21310408\\appdata\\roaming\\python\\python310\\site-packages (from gymnasium==0.29.1->gymnasium[ActionWrapper,accept-rom-license,box2d,classic_control]==0.29.1) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from gymnasium==0.29.1->gymnasium[ActionWrapper,accept-rom-license,box2d,classic_control]==0.29.1) (0.0.4)\n",
      "Requirement already satisfied: autorom~=0.4.2 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[ActionWrapper,accept-rom-license,box2d,classic_control]==0.29.1) (0.4.2)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from gymnasium[ActionWrapper,accept-rom-license,box2d,classic_control]==0.29.1) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from gymnasium[ActionWrapper,accept-rom-license,box2d,classic_control]==0.29.1) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from gymnasium[ActionWrapper,accept-rom-license,box2d,classic_control]==0.29.1) (4.3.0)\n",
      "Requirement already satisfied: click in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[ActionWrapper,accept-rom-license,box2d,classic_control]==0.29.1) (8.1.7)\n",
      "Requirement already satisfied: requests in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[ActionWrapper,accept-rom-license,box2d,classic_control]==0.29.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[ActionWrapper,accept-rom-license,box2d,classic_control]==0.29.1) (4.67.1)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[ActionWrapper,accept-rom-license,box2d,classic_control]==0.29.1) (0.6.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\21310408\\appdata\\roaming\\python\\python310\\site-packages (from click->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[ActionWrapper,accept-rom-license,box2d,classic_control]==0.29.1) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[ActionWrapper,accept-rom-license,box2d,classic_control]==0.29.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[ActionWrapper,accept-rom-license,box2d,classic_control]==0.29.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[ActionWrapper,accept-rom-license,box2d,classic_control]==0.29.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[ActionWrapper,accept-rom-license,box2d,classic_control]==0.29.1) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: gymnasium 0.29.1 does not provide the extra 'actionwrapper'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.10 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (3.12.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (18.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\21310408\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow==2.10) (24.2)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\21310408\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow==2.10) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\21310408\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow==2.10) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (1.17.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorflow==2.10) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.10) (0.44.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (2.36.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (3.1.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (3.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from matplotlib) (4.55.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\21310408\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\21310408\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\21310408\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\21310408\\appdata\\local\\anaconda3\\envs\\tf-env\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install -q -U gymnasium swig\n",
    "%pip install gymnasium[atari]==0.29.1\n",
    "%pip install gymnasium[classic_control,box2d,accept-rom-license,ActionWrapper]==0.29.1\n",
    "%pip install tensorflow==2.10\n",
    "%pip install \"numpy<2\"\n",
    "%pip install matplotlib\n",
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:38:14.554959Z",
     "start_time": "2024-12-10T14:38:11.859470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "GPUs available:\n",
      "Name: NVIDIA GeForce RTX 4060\n",
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(\"GPUs available:\")\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(\"Name:\", details.get('device_name', 'Unknown GPU'))\n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # set gpu 0 as default\n",
    "# Suppressing TensorFlow's informational and warning logs to avoid output clutter.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set TensorFlow to use only GPU 1\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:40:58.327484Z",
     "start_time": "2024-12-10T14:40:58.264785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(6)\n",
      "Observation space: Box(0, 255, (210, 160, 3), uint8)\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import gymnasium as gym\n",
    "\n",
    "# Creating the environment\n",
    "env = gym.make(\"ALE/Qbert-v5\", render_mode=\"rgb_array\", frameskip=4)\n",
    "\n",
    "# Inspecting the environment\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Observation space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:41:00.314343Z",
     "start_time": "2024-12-10T14:41:00.298417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed frame shape: (84, 84)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from collections import deque\n",
    "\n",
    "# Function to preprocess a single frame\n",
    "def preprocess_frame(frame, augment=False):\n",
    "    # Ensure the input is a NumPy array\n",
    "    if isinstance(frame, tuple):\n",
    "        frame = frame[0]  # Extract the observation if it's a tuple\n",
    "    frame = np.array(frame)  # Ensure it's a NumPy array\n",
    "    # Convert to grayscale\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    # Resize to 84x84\n",
    "    resized_frame = cv2.resize(gray_frame, (84, 84))\n",
    "\n",
    "    # Normalize the frame to the range [0, 1]\n",
    "    normalized_frame = resized_frame / 255.0\n",
    "\n",
    "    if augment:\n",
    "        # Random horizontal flip\n",
    "        if np.random.random() > 0.5:\n",
    "            normalized_frame = np.fliplr(normalized_frame)\n",
    "        # Add random noise\n",
    "        if np.random.random() > 0.7:\n",
    "            noise = np.random.normal(0, 0.01, normalized_frame.shape)\n",
    "            normalized_frame = np.clip(normalized_frame + noise, 0, 1)\n",
    "    return normalized_frame\n",
    "\n",
    "# Initialize frame stack\n",
    "frame_stack = deque(maxlen=4)\n",
    "\n",
    "# Reset environment and initialize stack\n",
    "def reset_env_with_stack(env):\n",
    "    state, info = env.reset()\n",
    "    processed_frame = preprocess_frame(state)\n",
    "    for _ in range(4):  # Stack 4 identical frames initially\n",
    "        frame_stack.append(processed_frame)\n",
    "    return np.stack(frame_stack, axis=-1), info  # Shape: (84, 84, 4)\n",
    "\n",
    "# Step the environment with frame stacking\n",
    "def step_env_with_stack(env, action):\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    processed_frame = preprocess_frame(next_state)\n",
    "    frame_stack.append(processed_frame)\n",
    "    return np.stack(frame_stack, axis=-1), reward, done, truncated, info\n",
    "\n",
    "state, _ = env.reset()\n",
    "processed_frame = preprocess_frame(state, augment=False)\n",
    "print(\"Processed frame shape:\", processed_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:41:01.855193Z",
     "start_time": "2024-12-10T14:41:01.776916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "state, _ = env.reset()\n",
    "processed_frame = preprocess_frame(state, augment=False)\n",
    "plt.imshow(processed_frame, cmap='gray')\n",
    "plt.title(\"Augmented Frame\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:48:29.413248Z",
     "start_time": "2024-12-10T14:48:29.252829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_25 (Conv2D)          (None, 21, 21, 32)        2080      \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 21, 21, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 10, 10, 64)        8256      \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 10, 10, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 10, 10, 64)        4160      \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 10, 10, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 6400)              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 64)                409664    \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 425,190\n",
      "Trainable params: 424,870\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the DQN model\n",
    "def create_dqn(action_space):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (4, 4), strides=4, activation='relu', input_shape=(84, 84, 4)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (2, 2), strides=2, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (1, 1), strides=1, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(action_space, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Initialize the model\n",
    "dqn_model = create_dqn(env.action_space.n)\n",
    "dqn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:50:20.065494Z",
     "start_time": "2024-12-10T14:50:20.049832Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(next_states),\n",
    "            np.array(dones)\n",
    "        )\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "replay_buffer = ReplayBuffer(capacity=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:50:21.460118Z",
     "start_time": "2024-12-10T14:50:21.444472Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def select_action(model, state, epsilon, action_space):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_space - 1)  # Random action\n",
    "    else:\n",
    "        q_values = model.predict(state[None, ...], verbose=0)\n",
    "        return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:50:26.608486Z",
     "start_time": "2024-12-10T14:50:26.592747Z"
    }
   },
   "outputs": [],
   "source": [
    "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "def record_gameplay(env, model, video_path=\"trained_agent_gameplay.mp4\"):\n",
    "    # Initialize video recorder\n",
    "    recorder = VideoRecorder(env, video_path)\n",
    "\n",
    "    # Initialize frame stack\n",
    "    frame_stack = deque(maxlen=4)\n",
    "\n",
    "    # Reset the environment\n",
    "    state, info = env.reset()\n",
    "    processed_frame = preprocess_frame(state)\n",
    "\n",
    "    # Stack the same frame 4 times initially\n",
    "    for _ in range(4):\n",
    "        frame_stack.append(processed_frame)\n",
    "\n",
    "    state = np.stack(frame_stack, axis=-1)  # Create the 4-channel input\n",
    "\n",
    "    for _ in range(500):  # Limit steps in the episode\n",
    "        # Predict action based on the current state\n",
    "        action = np.argmax(model.predict(state[None, ...], verbose=0))\n",
    "\n",
    "        # Take a step in the environment\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Record the frame\n",
    "        recorder.capture_frame()\n",
    "\n",
    "        # Preprocess the next frame and update the stack\n",
    "        processed_frame = preprocess_frame(next_state)\n",
    "        frame_stack.append(processed_frame)\n",
    "        state = np.stack(frame_stack, axis=-1)  # Update input state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Close the recorder and save the video\n",
    "    recorder.close()\n",
    "    recorder.enabled = False\n",
    "    print(f\"Gameplay recorded at recordings/{video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T16:04:09.378489Z",
     "start_time": "2024-12-10T15:08:31.599830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Reward: -175.00, Survival Time: 352, Epsilon: 0.980, Life Count: 0\n",
      "Episode 2/100, Reward: -235.00, Survival Time: 393, Epsilon: 0.960, Life Count: 0\n",
      "Episode 3/100, Reward: -350.00, Survival Time: 299, Epsilon: 0.941, Life Count: 0\n",
      "Episode 4/100, Reward: -325.00, Survival Time: 303, Epsilon: 0.922, Life Count: 0\n",
      "Episode 5/100, Reward: -200.00, Survival Time: 339, Epsilon: 0.904, Life Count: 0\n",
      "Episode 6/100, Reward: -275.00, Survival Time: 342, Epsilon: 0.886, Life Count: 0\n",
      "Episode 7/100, Reward: -350.00, Survival Time: 301, Epsilon: 0.868, Life Count: 0\n",
      "Episode 8/100, Reward: -325.00, Survival Time: 307, Epsilon: 0.851, Life Count: 0\n",
      "Episode 9/100, Reward: -405.00, Survival Time: 280, Epsilon: 0.834, Life Count: 0\n",
      "Episode 10/100, Reward: -250.00, Survival Time: 365, Epsilon: 0.817, Life Count: 0\n",
      "Episode 11/100, Reward: -225.00, Survival Time: 341, Epsilon: 0.801, Life Count: 0\n",
      "Episode 12/100, Reward: -350.00, Survival Time: 320, Epsilon: 0.785, Life Count: 0\n",
      "Episode 13/100, Reward: -75.00, Survival Time: 446, Epsilon: 0.769, Life Count: 0\n",
      "Episode 14/100, Reward: -375.00, Survival Time: 292, Epsilon: 0.754, Life Count: 0\n",
      "Episode 15/100, Reward: -325.00, Survival Time: 269, Epsilon: 0.739, Life Count: 0\n",
      "Episode 16/100, Reward: -325.00, Survival Time: 310, Epsilon: 0.724, Life Count: 0\n",
      "Episode 17/100, Reward: -300.00, Survival Time: 342, Epsilon: 0.709, Life Count: 0\n",
      "Episode 18/100, Reward: -200.00, Survival Time: 398, Epsilon: 0.695, Life Count: 0\n",
      "Episode 19/100, Reward: -325.00, Survival Time: 339, Epsilon: 0.681, Life Count: 0\n",
      "Episode 20/100, Reward: -300.00, Survival Time: 315, Epsilon: 0.668, Life Count: 0\n",
      "Episode 21/100, Reward: -300.00, Survival Time: 338, Epsilon: 0.654, Life Count: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 89\u001b[0m\n\u001b[0;32m     86\u001b[0m target_model \u001b[38;5;241m=\u001b[39m create_dqn(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[0;32m     87\u001b[0m target_model\u001b[38;5;241m.\u001b[39mset_weights(dqn_model\u001b[38;5;241m.\u001b[39mget_weights())  \u001b[38;5;66;03m# Synchronize weights\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m rewards_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdqn_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust for quick testing\u001b[39;49;00m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.98\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m    102\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[51], line 55\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[1;34m(env, model, target_model, replay_buffer, episodes, batch_size, gamma, epsilon, epsilon_min, epsilon_decay, update_target, verbose)\u001b[0m\n\u001b[0;32m     52\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39msample(batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Double DQN: Use the model to select actions, but the target model to evaluate Q-values\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m next_q_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     56\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m target_model\u001b[38;5;241m.\u001b[39mpredict(next_states, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m     57\u001b[0m target_q_values \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m next_q_values[np\u001b[38;5;241m.\u001b[39marange(batch_size), next_q_actions] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones)\n",
      "File \u001b[1;32mc:\\Users\\21310408\\AppData\\Local\\anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\21310408\\AppData\\Local\\anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\engine\\training.py:2220\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2211\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m   2212\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2213\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2214\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2217\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   2218\u001b[0m         )\n\u001b[1;32m-> 2220\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2224\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2228\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2230\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2231\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2233\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\21310408\\AppData\\Local\\anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\engine\\data_adapter.py:1582\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\21310408\\AppData\\Local\\anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\engine\\data_adapter.py:1262\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[0;32m   1261\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1277\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\21310408\\AppData\\Local\\anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\engine\\data_adapter.py:247\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    236\u001b[0m     x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    245\u001b[0m ):\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 247\u001b[0m     x, y, sample_weights \u001b[38;5;241m=\u001b[39m \u001b[43m_process_tensorlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m     sample_weight_modes \u001b[38;5;241m=\u001b[39m broadcast_sample_weight_modes(\n\u001b[0;32m    249\u001b[0m         sample_weights, sample_weight_modes\n\u001b[0;32m    250\u001b[0m     )\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;66;03m# If sample_weights are not specified for an output use 1.0 as weights.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21310408\\AppData\\Local\\anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\engine\\data_adapter.py:1142\u001b[0m, in \u001b[0;36m_process_tensorlike\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n\u001b[0;32m   1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m-> 1142\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_convert_single_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mlist_to_tuple(inputs)\n",
      "File \u001b[1;32mc:\\Users\\21310408\\AppData\\Local\\anaconda3\\envs\\tf-env\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\21310408\\AppData\\Local\\anaconda3\\envs\\tf-env\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\21310408\\AppData\\Local\\anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\engine\\data_adapter.py:1138\u001b[0m, in \u001b[0;36m_process_tensorlike.<locals>._convert_single_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1136\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mfloatx()\n\u001b[0;32m   1137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m-> 1138\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43m_is_scipy_sparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\21310408\\AppData\\Local\\anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\engine\\data_adapter.py:1863\u001b[0m, in \u001b[0;36m_is_scipy_sparse\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_scipy_sparse\u001b[39m(x):\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1863\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[0;32m   1865\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m issparse(x)\n\u001b[0;32m   1866\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1411\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1544\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_dqn(env, model, target_model, replay_buffer, episodes=500, batch_size=32, gamma=0.99, epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.995, update_target=10, verbose=0):\n",
    "    \"\"\"\n",
    "    Train the DQN model using survival-based rewards.\n",
    "    \"\"\"\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    loss_fn = tf.keras.losses.Huber()\n",
    "    rewards_history = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = reset_env_with_stack(env)\n",
    "        total_reward = 0\n",
    "        episode_loss = 0\n",
    "        last_lives = 4\n",
    "        num_of_NOOPs = 0\n",
    "\n",
    "        for step in range(10_000):  # Limit steps per episode\n",
    "            action = select_action(model, state, epsilon, env.action_space.n)\n",
    "            next_state, reward, done, truncated, info = step_env_with_stack(env, action)\n",
    "\n",
    "            # # Calculate adjusted reward\n",
    "            # base_reward = 1  # Reward for surviving\n",
    "            # penalty = -120 / survival_time if done else 0  # Penalize more for short survival\n",
    "            # bonus = 15 if reward > 0  else 0\n",
    "            # adjusted_reward = base_reward + reward + penalty + bonus\n",
    "\n",
    "            # if survival_time == 350:  # Encourage longer survival\n",
    "            #     adjusted_reward += 25\n",
    "\n",
    "            adjusted_reward = reward\n",
    "            if info.get(\"lives\") < last_lives:\n",
    "                last_lives = info.get(\"lives\")\n",
    "                adjusted_reward -= 100\n",
    "\n",
    "            if action == 0:\n",
    "                num_of_NOOPs += 1\n",
    "                if num_of_NOOPs >= 3:\n",
    "                    adjusted_reward -= 5\n",
    "            else:\n",
    "                num_of_NOOPs = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Store adjusted reward in replay buffer\n",
    "            replay_buffer.add((state, action, adjusted_reward, next_state, done))\n",
    "            state = next_state\n",
    "            total_reward += adjusted_reward\n",
    "\n",
    "            if replay_buffer.size() > batch_size:\n",
    "                # Sample from the replay buffer\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size=batch_size)\n",
    "\n",
    "                # Double DQN: Use the model to select actions, but the target model to evaluate Q-values\n",
    "                next_q_actions = np.argmax(model.predict(next_states, verbose=verbose), axis=1)\n",
    "                next_q_values = target_model.predict(next_states, verbose=verbose)\n",
    "                target_q_values = rewards + gamma * next_q_values[np.arange(batch_size), next_q_actions] * (1 - dones)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    q_values = model(states)\n",
    "                    q_values = tf.reduce_sum(q_values * tf.one_hot(actions, env.action_space.n), axis=1)\n",
    "                    loss = loss_fn(target_q_values, q_values)\n",
    "                    episode_loss += loss.numpy()\n",
    "\n",
    "                grads = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if episode == 40:\n",
    "            epsilon = min(0.3, epsilon * epsilon_decay)  # Drop epsilon to 0.3 after episode 40\n",
    "        else:\n",
    "            # Update epsilon for exploration-exploitation tradeoff\n",
    "            epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        # Update target model weights periodically\n",
    "        if episode % update_target == 0:\n",
    "            target_model.set_weights(model.get_weights())\n",
    "\n",
    "        rewards_history.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Reward: {total_reward:.2f}, Survival Time: {step}, Epsilon: {epsilon:.3f}, Death Counter: {last_lives}\")\n",
    "\n",
    "    return rewards_history\n",
    "\n",
    "target_model = create_dqn(env.action_space.n)\n",
    "target_model.set_weights(dqn_model.get_weights())  # Synchronize weights\n",
    "\n",
    "rewards_history = train_dqn(\n",
    "    env=env,\n",
    "    model=dqn_model,\n",
    "    target_model=target_model,\n",
    "    replay_buffer=replay_buffer,\n",
    "    episodes=100,  # Adjust for quick testing\n",
    "    batch_size=32,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_min=0.1,\n",
    "    epsilon_decay=0.98,\n",
    "    update_target=5,\n",
    "    verbose=0\n",
    ")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_gameplay(env, dqn_model, video_path=\"trained_agent_gameplay.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's weights\n",
    "dqn_model.save_weights(\"models/dqn_model_weights.h5\")\n",
    "print(\"Model weights saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T06:13:34.891012Z",
     "start_time": "2024-12-10T06:13:34.805497Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "dqn_model.save(\"models/dqn_model.h5\")\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T13:44:23.063705Z",
     "start_time": "2024-12-10T13:43:28.665797Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, model, episodes=10):\n",
    "    total_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state, _ = reset_env_with_stack(env)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(model.predict(state[None, ...], verbose=0))\n",
    "            next_state, reward, done, truncated, _ = step_env_with_stack(env, action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}, Reward: {total_reward}\")\n",
    "\n",
    "    print(f\"Average Reward: {np.mean(total_rewards)}\")\n",
    "\n",
    "evaluate_agent(env, dqn_model, episodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T13:47:19.945882Z",
     "start_time": "2024-12-10T13:47:19.882484Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_rewards(rewards):\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Training Rewards')\n",
    "    plt.show()\n",
    "\n",
    "# Example\n",
    "plot_rewards(rewards_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T13:47:26.137907Z",
     "start_time": "2024-12-10T13:47:26.090493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate key statistics\n",
    "average_reward = np.mean(rewards_history)\n",
    "max_reward = np.max(rewards_history)\n",
    "\n",
    "# Plot the rewards with highlights\n",
    "plt.plot(rewards_history, label=\"Raw Rewards\")\n",
    "plt.axhline(average_reward, color='r', linestyle='--', label=f\"Average Reward: {average_reward:.2f}\")\n",
    "plt.axhline(max_reward, color='g', linestyle='--', label=f\"Max Reward: {max_reward}\")\n",
    "plt.title(\"Training Rewards with Statistics\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T06:20:48.462414Z",
     "start_time": "2024-12-10T06:20:48.186683Z"
    }
   },
   "outputs": [],
   "source": [
    "window = 10  # Adjust the window size as needed\n",
    "smoothed_rewards = np.convolve(rewards_history, np.ones(window)/window, mode='valid')\n",
    "\n",
    "# Plot smoothed rewards\n",
    "plt.plot(smoothed_rewards)\n",
    "plt.title(\"Smoothed Training Rewards\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T06:21:27.632894Z",
     "start_time": "2024-12-10T06:21:27.334487Z"
    }
   },
   "outputs": [],
   "source": [
    "# Segment rewards into blocks\n",
    "block_size = 10\n",
    "block_means = [np.mean(rewards_history[i:i+block_size]) for i in range(0, len(rewards_history), block_size)]\n",
    "\n",
    "# Plot block means\n",
    "plt.plot(block_means)\n",
    "plt.title(\"Mean Reward Per Block of Episodes\")\n",
    "plt.xlabel(\"Blocks (10 episodes each)\")\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T06:21:04.444648Z",
     "start_time": "2024-12-10T06:21:04.047819Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate cumulative rewards\n",
    "cumulative_rewards = np.cumsum(rewards_history)\n",
    "\n",
    "# Plot cumulative rewards\n",
    "plt.plot(cumulative_rewards)\n",
    "plt.title(\"Cumulative Training Rewards\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
