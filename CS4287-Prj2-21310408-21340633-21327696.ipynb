{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Project Part 3\n",
    "\n",
    "Code runs through until completion\n",
    "\n",
    "## Contributors: \n",
    "- Tadhg Ryan (21310408)\n",
    "- Craig Phayer (21340633)\n",
    "- Thomas McCarty (21327696)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install -q -U gymnasium swig\n",
    "%pip install gymnasium[atari]==0.29.1\n",
    "%pip install gymnasium[classic_control,box2d,accept-rom-license,ActionWrapper]==0.29.1\n",
    "%pip install tensorflow==2.10\n",
    "%pip install \"numpy<2\"\n",
    "%pip install matplotlib\n",
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:38:14.554959Z",
     "start_time": "2024-12-10T14:38:11.859470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "No GPU available.\n",
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(\"GPUs available:\")\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(\"Name:\", details.get('device_name', 'Unknown GPU'))\n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # set gpu 0 as default\n",
    "# Suppressing TensorFlow's informational and warning logs to avoid output clutter.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set TensorFlow to use only GPU 1\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:40:58.327484Z",
     "start_time": "2024-12-10T14:40:58.264785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(6)\n",
      "Observation space: Box(0, 255, (210, 160, 3), uint8)\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import gymnasium as gym\n",
    "\n",
    "# Creating the environment\n",
    "env = gym.make(\"ALE/Qbert-v5\", render_mode=\"rgb_array\", frameskip=4) # TODO: Check render_mode, env.reset(seed=seed)\n",
    "\n",
    "# Inspecting the environment\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Observation space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:41:00.314343Z",
     "start_time": "2024-12-10T14:41:00.298417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed frame shape: (84, 84)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from collections import deque\n",
    "\n",
    "# TODO: Test AtariPreprocessing (FrameStacking + grayscaling + resizing)\n",
    "# Function to preprocess a single frame\n",
    "def preprocess_frame(frame, augment=False):\n",
    "    # Ensure the input is a NumPy array\n",
    "    if isinstance(frame, tuple):\n",
    "        frame = frame[0]  # Extract the observation if it's a tuple\n",
    "    frame = np.array(frame)  # Ensure it's a NumPy array\n",
    "    # Convert to grayscale\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    # Resize to 84x84\n",
    "    resized_frame = cv2.resize(gray_frame, (84, 84))\n",
    "\n",
    "    # Normalize the frame to the range [0, 1]\n",
    "    normalized_frame = resized_frame / 255.0\n",
    "\n",
    "    if augment:\n",
    "        # Random horizontal flip\n",
    "        if np.random.random() > 0.5:\n",
    "            normalized_frame = np.fliplr(normalized_frame)\n",
    "        # Add random noise\n",
    "        if np.random.random() > 0.7:\n",
    "            noise = np.random.normal(0, 0.01, normalized_frame.shape)\n",
    "            normalized_frame = np.clip(normalized_frame + noise, 0, 1)\n",
    "    return normalized_frame\n",
    "\n",
    "# Initialize frame stack\n",
    "frame_stack = deque(maxlen=4)\n",
    "\n",
    "# Reset environment and initialize stack\n",
    "def reset_env_with_stack(env):\n",
    "    state, info = env.reset()\n",
    "    processed_frame = preprocess_frame(state)\n",
    "    for _ in range(4):  # Stack 4 identical frames initially\n",
    "        frame_stack.append(processed_frame)\n",
    "    return np.stack(frame_stack, axis=-1), info  # Shape: (84, 84, 4)\n",
    "\n",
    "# Step the environment with frame stacking\n",
    "def step_env_with_stack(env, action):\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    processed_frame = preprocess_frame(next_state)\n",
    "    frame_stack.append(processed_frame)\n",
    "    return np.stack(frame_stack, axis=-1), reward, done, truncated, info\n",
    "\n",
    "state, _ = env.reset()\n",
    "processed_frame = preprocess_frame(state, augment=False)\n",
    "print(\"Processed frame shape:\", processed_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:41:01.855193Z",
     "start_time": "2024-12-10T14:41:01.776916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/GU6VOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5zUlEQVR4nO3de1gV1d4H8O+Wuxc2grCRFET05AU9GSYidhSleD3W0aTUXjti+ZQakOjTKTlveDlesLvlBavjAe1glG+pvXqyRxHsVREvZUfzjbygcFIwL+xNXoBg3j963MeZ2bJnw8DaG7+f55nnaa1ZM7NmzY6fM2vWLIMkSRKIiIhaWTvRFSAiorsTAxAREQnBAEREREIwABERkRAMQEREJAQDEBERCcEAREREQjAAERGREAxAREQkBAMQkQvKycmBwWDA2bNnRVeFqMkYgEh3a9asgcFgQHR0tOiqCHX9+nUsXLgQhYWFwuqwcOFCGAwGm8vatWuF1YsIANxFV4DantzcXPTo0QMHDx7EqVOn0KtXL9FVEuL69etYtGgRAGDkyJFC65KVlYWOHTvK8u72fyCQeAxApKvS0lLs378fn332GWbMmIHc3FwsWLBAdLXueo8//ji6dOmiqey1a9fQoUOHFq4RER/Bkc5yc3PRuXNnjB07Fo8//jhyc3NVZQoLC2EwGFSPps6ePQuDwYCcnBxZ/qZNm9CvXz94e3sjMjISmzdvxrRp09CjRw/Vtm+88QZWr16Nnj17on379nj44YdRXl4OSZKwePFidOvWDT4+Phg3bhyuXLmiqtsXX3yBBx98EB06dECnTp0wduxYfPfdd7Iy06ZNQ8eOHfHjjz9i/Pjx6NixIwIDA/Hiiy+ivr7eWp/AwEAAwKJFi6yPvRYuXGjdz/fff4/HH38c/v7+8Pb2xuDBg/H555+r6vTdd99h1KhR8PHxQbdu3bBkyRI0NDQ0dhk0u9WXtGfPHjz//PMICgpCt27dAADnzp3D888/j3vvvRc+Pj4ICAjAE088oep3urWPvXv34oUXXkBgYCD8/PwwY8YM1NbWoqqqClOnTkXnzp3RuXNnvPTSS1B+hL+hoQErVqxA//794e3tDZPJhBkzZuDq1au6nCc5J94Bka5yc3MxYcIEeHp64sknn0RWVhYOHTqEBx54oEn72759OyZNmoQBAwYgMzMTV69exfTp03HPPffc8fi1tbVITU3FlStX8Nprr2HixIkYNWoUCgsL8fLLL+PUqVNYuXIlXnzxRfztb3+zbvvhhx8iKSkJCQkJePXVV3H9+nVkZWVh+PDh+Oabb2QBr76+HgkJCYiOjsYbb7yBXbt24c0330RERARmzZqFwMBAZGVlYdasWXjssccwYcIEAMDAgQMB/BpUYmNjcc8992DevHno0KEDPvnkE4wfPx6ffvopHnvsMQBARUUF4uLi8Msvv1jLvf/++/Dx8XGoHZXB1s3NDZ07d7amn3/+eQQGBmL+/Pm4du0aAODQoUPYv38/Jk+ejG7duuHs2bPIysrCyJEjceLECbRv3162z9TUVAQHB2PRokU4cOAA3n//ffj5+WH//v0IDQ3FsmXL8I9//AOvv/46IiMjMXXqVOu2M2bMQE5ODp5++mm88MILKC0txapVq/DNN99g37598PDwcOh8yUVIRDo5fPiwBEDauXOnJEmS1NDQIHXr1k2aPXu2rFxBQYEEQCooKJDll5aWSgCk7Oxsa96AAQOkbt26SdXV1da8wsJCCYAUFham2jYwMFCqqqqy5qenp0sApN/+9rdSXV2dNf/JJ5+UPD09pZs3b0qSJEnV1dWSn5+f9Oyzz8rqVFFRIRmNRll+UlKSBED6y1/+Iis7aNAgKSoqypr+6aefJADSggULVG01evRoacCAAdbj32qvYcOGSb1797bmpaWlSQCk4uJia97Fixclo9EoAZBKS0tV+77dggULJACq5VbbZWdnSwCk4cOHS7/88ots2+vXr6v2V1RUJAGQNmzYYM27tY+EhASpoaHBmh8TEyMZDAZp5syZ1rxffvlF6tatmzRixAhr3v/+7/9KAKTc3FzZsXbs2GEzn9oOPoIj3eTm5sJkMiEuLg4AYDAYMGnSJOTl5VkfTTni/PnzOHbsGKZOnSrrQB8xYgQGDBhgc5snnngCRqPRmr7V0f7UU0/B3d1dll9bW4sff/wRALBz505UVVXhySefxKVLl6yLm5sboqOjUVBQoDrWzJkzZekHH3wQZ86csXteV65cwe7duzFx4kRUV1dbj3X58mUkJCTg5MmT1nr94x//wNChQzFkyBDr9oGBgZgyZYrd49zu008/xc6dO62L8tHos88+Czc3N1ne7XdZdXV1uHz5Mnr16gU/Pz98/fXXqmNMnz4dBoPBmo6OjoYkSZg+fbo1z83NDYMHD5a106ZNm2A0GvHQQw/J2j4qKgodO3a02fbUNvARHOmivr4eeXl5iIuLQ2lpqTU/Ojoab775JvLz8/Hwww87tM9z584BgM236Hr16mXzj2BoaKgsfSsYde/e3Wb+rT6GkydPAgBGjRplsy6+vr6ytLe3t7WP55bOnTtr6rM4deoUJElCRkYGMjIybJa5ePEi7rnnHpw7d87m22r33nuv3ePc7ne/+12jLyGEh4er8m7cuIHMzExkZ2fjxx9/lPXbmM1mVXlH2v72djp58iTMZjOCgoJs1u3ixYt3rDe5NgYg0sXu3btx4cIF5OXlIS8vT7U+NzfXGoBu/1fy7Zpyl6Sk/Fe8vfxbf1Rvdep/+OGHCA4OVpW7/e6psf1pcetYL774IhISEmyWae1X1231KaWmpiI7OxtpaWmIiYmB0WiEwWDA5MmTbb4E4Ujb3x7MGhoaEBQUZPOFFQCqQE9tBwMQ6SI3NxdBQUFYvXq1at1nn32GzZs3Y+3atfDx8bF2fldVVcnK3brjuSUsLAzAr3cMSrbymiMiIgIAEBQUhPj4eF32eadA27NnTwCAh4eH3WOFhYVZ785uV1JS0vwK2vHf//3fSEpKwptvvmnNu3nzpuq6NVdERAR27dqF2NhYh1+uINfGPiBqths3buCzzz7DI488gscff1y1pKSkoLq62vqKcVhYGNzc3PDVV1/J9rNmzRpZOiQkBJGRkdiwYQN+/vlna/6ePXtw7NgxXc8hISEBvr6+WLZsGerq6lTrf/rpJ4f3eestMeUf7KCgIIwcORLvvfceLly40Oixfv/73+PAgQM4ePCgbP2d7hb05ObmpnpdeuXKlbrcqd5u4sSJqK+vx+LFi1XrfvnlF90DHjkP3gFRs33++eeorq7GH/7wB5vrhw4disDAQOTm5mLSpEkwGo144oknsHLlShgMBkRERGDbtm02n/UvW7YM48aNQ2xsLJ5++mlcvXoVq1atQmRkpCwoNZevry+ysrLwxz/+Effffz8mT56MwMBAlJWVYfv27YiNjcWqVasc2qePjw/69euHjz/+GL/5zW/g7++PyMhIREZGYvXq1Rg+fDgGDBiAZ599Fj179kRlZSWKiorwr3/9C99++y0A4KWXXsKHH36I//iP/8Ds2bOtr2GHhYXhn//8p27nb8sjjzyCDz/8EEajEf369UNRURF27dqFgIAAXY8zYsQIzJgxA5mZmTh69CgefvhheHh44OTJk9i0aRPeeecdPP7447oek5wDAxA1W25uLry9vfHQQw/ZXN+uXTuMHTsWubm5uHz5MgICArBy5UrU1dVh7dq18PLywsSJE63jQ2736KOP4qOPPsLChQsxb9489O7dGzk5OVi/fr1qgGhz/ed//idCQkKwfPlyvP7666ipqcE999yDBx98EE8//XST9vnXv/4VqampmDNnDmpra7FgwQJERkaiX79+OHz4MBYtWoScnBxcvnwZQUFBGDRoEObPn2/dvmvXrigoKEBqaiqWL1+OgIAAzJw5EyEhIbK3y1rCO++8Azc3N+Tm5uLmzZuIjY3Frl277thv1Rxr165FVFQU3nvvPfz5z3+Gu7s7evTogaeeegqxsbG6H4+cg0FS3mMTuYD77rsPgYGB2Llzp+iqEFETsQ+InFpdXR1++eUXWV5hYSG+/fZb4R/4JKLm4R0QObWzZ88iPj4eTz31FEJCQvD9999j7dq1MBqNOH78uO79EUTUetgHRE6tc+fOiIqKwl//+lf89NNP6NChA8aOHWvtDyEi18U7ICIiEoJ9QEREJESLBaDVq1ejR48e8Pb2RnR0tGwgHRERUYs8gvv4448xdepUrF27FtHR0VixYgU2bdqEkpKSO35w8JaGhgacP38enTp1uuOnTIiIyHlJkoTq6mqEhISgXbtG7nNaYo6HIUOGSMnJydZ0fX29FBISImVmZtrdtry83Ob8JVy4cOHCxbWW8vLyRv/e6/4Irra2FkeOHJF9ZLFdu3aIj49HUVGRqnxNTQ0sFot1kfhOBBFRm9CpU6dG1+segC5duoT6+nqYTCZZvslkQkVFhap8ZmYmjEajdVHOKUJERK7JXjeK8Lfg0tPTYTabrUt5ebnoKhERUSvQfSBqly5d4ObmhsrKSll+ZWWlzYm+vLy84OXlpXc1iIjIyel+B+Tp6YmoqCjk5+db8xoaGpCfn4+YmBi9D0dERC6qRT7FM3fuXCQlJWHw4MEYMmQIVqxYgWvXrjX5k/ZERNT2tEgAmjRpEn766SfMnz8fFRUVuO+++7Bjxw7ViwlERHT3crpvwVksFhiNRtHVICKiZjKbzfD19b3jeuFvwRER0d2JAYiIiIRgACIiIiEYgIiISAgGICIiEoIBiIiIhGAAIiIiIRiAiIhICAYgIiISggGIiIiEYAAiIiIhGICIiEgIBiAiIhKCAYiIiIRgACIiIiEYgIiISAgGICIiEoIBiIiIhGAAIiIiIRiAiIhICAYgIiISggGIiIiEYAAiIiIhGICIiEgIBiAiIhKCAYiIiIRgACIiIiEYgIiISAgGICIiEoIBiIiIhGAAIiIiIRiAiIhICAYgIiISwuEA9NVXX+HRRx9FSEgIDAYDtmzZIlsvSRLmz5+Prl27wsfHB/Hx8Th58qRe9SUiojbC4QB07do1/Pa3v8Xq1attrn/ttdfw7rvvYu3atSguLkaHDh2QkJCAmzdvNruyRETUhkjNAEDavHmzNd3Q0CAFBwdLr7/+ujWvqqpK8vLykj766CNN+zSbzRIALly4cOHi4ovZbG70772ufUClpaWoqKhAfHy8Nc9oNCI6OhpFRUU2t6mpqYHFYpEtRETU9ukagCoqKgAAJpNJlm8ymazrlDIzM2E0Gq1L9+7d9awSERE5KeFvwaWnp8NsNluX8vJy0VUiIqJWoGsACg4OBgBUVlbK8isrK63rlLy8vODr6ytbiIio7dM1AIWHhyM4OBj5+fnWPIvFguLiYsTExOh5KCIicnHujm7w888/49SpU9Z0aWkpjh49Cn9/f4SGhiItLQ1LlixB7969ER4ejoyMDISEhGD8+PF61puIiFydo69eFxQU2HzdLikpyfoqdkZGhmQymSQvLy9p9OjRUklJieb98zVsLly4cGkbi73XsA2SJElwIhaLBUajUXQ1iIiomcxmc6P9+sLfgiMiorsTAxAREQnBAEREREI4/BYcETUuIiLCbpnTp0+3Qk2InBvvgIiISAgGICIiEoIBiIiIhGAAIiIiIfgSAt21OnXqpMpLSkpqdJtevXq1VHVUbv/k1Z2sWrWqFWpC1DJ4B0REREIwABERkRAMQEREJAT7gOiuVVNTo8o7fvx4o9to6XNJSUmxW0bLfkaOHGm3DJEr4x0QEREJwQBERERCMAAREZEQ7AOiu1ZwcLAqr6CgoNFtCgsL7e43Li7Obhl7xwGARYsW2S1D5Mp4B0REREIwABERkRAMQEREJAQDEBERCcGXEIgcoGVwqCRJdstoeZmBqK3jHRAREQnBAEREREIwABERkRDsAyISQMsg0wULFtgtw74kcmW8AyIiIiEYgIiISAgGICIiEoIBiIiIhGAAIiIiIRiAiIhICAYgIiISwqEAlJmZiQceeACdOnVCUFAQxo8fj5KSElmZmzdvIjk5GQEBAejYsSMSExNRWVmpa6WJiMj1OTQQdc+ePUhOTsYDDzyAX375BX/+85/x8MMP48SJE+jQoQMAYM6cOdi+fTs2bdoEo9GIlJQUTJgwAfv27WuREyByRVoGmXJGVGrrHApAO3bskKVzcnIQFBSEI0eO4He/+x3MZjPWrVuHjRs3YtSoUQCA7Oxs9O3bFwcOHMDQoUP1qzkREbm0ZvUBmc1mAIC/vz8A4MiRI6irq0N8fLy1TJ8+fRAaGoqioiKb+6ipqYHFYpEtRETU9jU5ADU0NCAtLQ2xsbGIjIwEAFRUVMDT0xN+fn6ysiaTCRUVFTb3k5mZCaPRaF26d+/e1CoREZELaXIASk5OxvHjx5GXl9esCqSnp8NsNluX8vLyZu2PiIhcQ5O+hp2SkoJt27bhq6++Qrdu3az5wcHBqK2tRVVVlewuqLKyEsHBwTb35eXlBS8vr6ZUg4iIXJhDd0CSJCElJQWbN2/G7t27ER4eLlsfFRUFDw8P5OfnW/NKSkpQVlaGmJgYfWpMRERtgkN3QMnJydi4cSO2bt2KTp06Wft1jEYjfHx8YDQaMX36dMydOxf+/v7w9fVFamoqYmJi+AYcERHJOBSAsrKyAAAjR46U5WdnZ2PatGkAgLfffhvt2rVDYmIiampqkJCQgDVr1uhSWSIiajsMkiRJoitxO4vFAqPRKLoadBcIDQ1V5Z07d65Vjh0XF2e3jJbBqlr2QySK2WyGr6/vHdfzW3BERCQEAxAREQnBAEREREI0aRwQUVtQVlamyjMYDI1u05pdpuzfobaOd0BERCQEAxAREQnBAEREREIwABERkRB8CYHIAfZeUgDUXwqxpbCwsPmVIXJxvAMiIiIhGICIiEgIBiAiIhKCfUDkdLp27SpLt2/fXlVmyJAhsnR0dHSL1umWlStX2i2jV/9ORESE3TKpqam6HMue4uJiWfrgwYOqMtevX5elL1y40KJ1ItfHOyAiIhKCAYiIiIRgACIiIiEYgIiISAjOiEouyd/fX5bu3LmzLN2zZ0/VNmPHjpWlLRaLqsz69esbPe7p06e1VrFV2HtRISkpSZWnnKFy+/btqjJnzpyRpa9evSpLX7lyRWsV6S7GGVGJiMgpMQAREZEQDEBERCQE+4DI6WzZskWWHjdunKpMTk6OLK3su7l06ZJqm+PHj8vSnp6eqjLDhg1rtG4LFixodD2g30ymBQUFdsssWrSo0fX79+9X5dXW1srSkZGRqjJdunSRpZV9SdOmTVNts3XrVll6/PjxjdaN2j72ARERkVNiACIiIiEYgIiISAh+jJRckrIPQpnW0gfUFPb6XPSk5Vha+qTs0dIHRNQSeAdERERCMAAREZEQDEBERCQEAxAREQnBlxCoTbLViT5y5EhZWjkgE7A9cNOV2RpYa2sALpEIvAMiIiIhGICIiEgIhwJQVlYWBg4cCF9fX/j6+iImJgZffPGFdf3NmzeRnJyMgIAAdOzYEYmJiaisrNS90kRE5Poc6gPq1q0bli9fjt69e0OSJKxfvx7jxo3DN998g/79+2POnDnYvn07Nm3aBKPRiJSUFEyYMAH79u1rqfoTNVlVVZUqz97gTy0DPwsLC5tYI8ePZa++H3/8sSovKCioyXUi0pNDAejRRx+VpZcuXYqsrCwcOHAA3bp1w7p167Bx40aMGjUKAJCdnY2+ffviwIEDGDp0qH61JiIil9fkPqD6+nrk5eXh2rVriImJwZEjR1BXV4f4+HhrmT59+iA0NBRFRUV33E9NTQ0sFotsISKits/hAHTs2DF07NgRXl5emDlzJjZv3ox+/fqhoqICnp6e8PPzk5U3mUyoqKi44/4yMzNhNBqtS/fu3R0+CSIicj0OB6B7770XR48eRXFxMWbNmoWkpCScOHGiyRVIT0+H2Wy2LuXl5U3eFxERuQ6HB6J6enqiV69eAICoqCgcOnQI77zzDiZNmoTa2lpUVVXJ7oIqKysRHBx8x/15eXnBy8vL8ZoTEZFLa/Y4oIaGBtTU1CAqKgoeHh7Iz8+3rispKUFZWRliYmKaexgiImpjHLoDSk9Px5gxYxAaGorq6mps3LgRhYWF+PLLL2E0GjF9+nTMnTsX/v7+8PX1RWpqKmJiYvgGHBERqTgUgC5evIipU6fiwoULMBqNGDhwIL788ks89NBDAIC3334b7dq1Q2JiImpqapCQkIA1a9a0SMWJiMi1ORSA1q1b1+h6b29vrF69GqtXr25WpYhag/KNTcD+4E9XmxHV1jkSOQt+C46IiIRgACIiIiEYgIiISAgGICIiEoIBiIiIhGAAIiIiIRiAiIhICAYgIiISwuGPkRK1FU2ZEdXZtNaMqDk5ObL0+vXrVWUuXbrk8H7p7sY7ICIiEoIBiIiIhGAAIiIiIQySJEmiK3E7i8UCo9Eouhrk4saNG6fK27JliyxdVlamKhMWFtZSVRLi3LlzqrzQ0FBZevz48aoyW7dubakq0V3EbDbD19f3jut5B0REREIwABERkRAMQEREJAQDEBERCcGBqNSqpk6dKkvb6qDctm2bLH327FmHj7Nv3z5VXlxcnCx948YNh/erRUpKit0yq1atapFjK02cOFGV5+PjI0sfP35cl2P16NFDln7kkUdUZSwWiyy9YcMGXY5Nrol3QEREJAQDEBERCcEAREREQnAgKmmipV+jV69erVCTX12+fFmWvnr1qix9+vRp1TZffPGFLN2pUydVmaSkpEaP25rneOrUKbtl9OhLGjNmjCovIiJClu7cubMsHRAQ0OzjatVa7UD640BUIiJySgxAREQkBAMQEREJwXFApEliYqLdMspxNrYox4q4ubmpylRUVMjS165dU5WZNm2aLK3su9m7d69qG2UfkHI8DGD/PLWco7L/xBZbfVRKBQUFdsvo0fcxbNgwVd7w4cNlaeUEdIsXL1Zt06FDB1k6ODhYVaa+vl6W1jLGq7XagVof74CIiEgIBiAiIhKCAYiIiIRgACIiIiE4EJU00etnopxx1NaspMoObmWHOAAUFhbK0nv27JGlL126pNpG+dFN5cyggO0ZRB1lMBjsltHSsb5o0SK7ZZTt0BSRkZGqvC5dusjSI0aMkKVHjhyp2kb54kdGRoaqjLLN9WhvQFubU+vjQFQiInJKDEBERCREswLQ8uXLYTAYkJaWZs27efMmkpOTERAQgI4dOyIxMRGVlZXNrScREbUxTR6IeujQIbz33nsYOHCgLH/OnDnYvn07Nm3aBKPRiJSUFEyYMMHmBGF099HyzP+VV16RpW31hSj7JBYsWCBL2xqIqtfEa/Zo6S/TMqBVeU626NEHNGnSJFWest9NeRxb1yQ2NlaWdrLuZXJCTboD+vnnnzFlyhR88MEHsq/kms1mrFu3Dm+99RZGjRqFqKgoZGdnY//+/Thw4IBulSYiItfXpACUnJyMsWPHIj4+XpZ/5MgR1NXVyfL79OmD0NBQFBUV2dxXTU0NLBaLbCEiorbP4UdweXl5+Prrr3Ho0CHVuoqKCnh6esLPz0+WbzKZVN/3uiUzM1PT66ZERNS2OHQHVF5ejtmzZyM3Nxfe3t66VCA9PR1ms9m6lJeX67JfIiJybg4FoCNHjuDixYu4//774e7uDnd3d+zZswfvvvsu3N3dYTKZUFtbi6qqKtl2lZWVNr+MCwBeXl7w9fWVLURE1PY59Ahu9OjROHbsmCzv6aefRp8+ffDyyy+je/fu8PDwQH5+vvWz9iUlJSgrK0NMTIx+tSYiIpfnUADq1KmT6rMdHTp0QEBAgDV/+vTpmDt3Lvz9/eHr64vU1FTExMRg6NCh+tWaiIhcnu4T0r399tto164dEhMTUVNTg4SEBKxZs0bvwxARkYvjx0hJk9b8mSgHjNr6sKgzf4xUCy0DSF3tY6TKbWztt6XwY6TOiR8jJSIip8QAREREQjAAERGREAxAREQkBAMQEREJwQBERERCMAAREZEQDEBERCSE7l9CIGquvLw8WdrWbLrOPCOqFloGmbr6jKhLlixpdt2obeMdEBERCcEAREREQjAAERGREOwDIpfUo0cPWVr5cUyz2dx6lWkhtj742RIGDBhg99hnz56VpZUffyVqCt4BERGREAxAREQkBAMQEREJwQBERERCcEZU0o1ePyUtM6KuX79els7JydHl2PbodY5aBpDGxcXpciw9TJs2TZZOSkpSlWmpGVE526nr4oyoRETklBiAiIhICAYgIiISgn1A1KrOnTsnS4eGhqrKjB8/XpbeunVrS1aJdDJu3DhZesuWLaoyZWVlsnRYWFhLVokEYx8QERE5JQYgIiISggGIiIiEYB/QXSAlJaXR9b169bK7j1OnTtkts2rVKs11aq4hQ4Y0mm7fvr1qm5CQEFm6rq5OVUbZR6XUmueohb1ra6uPxcPDQ5Y+f/68qsz169dl6YMHDzaabkn2zhGw/xt2tt/v3YJ9QERE5JQYgIiISAgGICIiEoIBiIiIhGAAIiIiIRiAiIhICAYgIiISwqEAtHDhQhgMBtnSp08f6/qbN28iOTkZAQEB6NixIxITE1FZWal7pYmIyPW5O7pB//79sWvXrn/vwP3fu5gzZw62b9+OTZs2wWg0IiUlBRMmTMC+ffv0qS2paBlH3FoTemmpi3JgpPLjlID6I5bKj1wCv/5j6HaLFi2yX0EdtGZ7O9O1XbBggSxdXFysKqP8aKzyo7KA+uOz9gb+As71++XkePpyOAC5u7sjODhYlW82m7Fu3Tps3LgRo0aNAgBkZ2ejb9++OHDgAIYOHdr82hIRUZvhcB/QyZMnERISgp49e2LKlCnWf8EeOXIEdXV1iI+Pt5bt06cPQkNDUVRUdMf91dTUwGKxyBYiImr7HApA0dHRyMnJwY4dO5CVlYXS0lI8+OCDqK6uRkVFBTw9PeHn5yfbxmQyoaKi4o77zMzMhNFotC7du3dv0okQEZFrcegR3JgxY6z/PXDgQERHRyMsLAyffPIJfHx8mlSB9PR0zJ0715q2WCwMQkREdwGH+4Bu5+fnh9/85jc4deoUHnroIdTW1qKqqkp2F1RZWWmzz+gWLy8veHl5NacaZIe9ztW4uDhdjlNYWGi3zM2bN3U5lvIlBGV67969qm0yMjKafVwt56gXLccaOXJks4+zePFiVd7w4cObvV9blNe/tc6xoKCg2fsg/TVrHNDPP/+M06dPo2vXroiKioKHhwfy8/Ot60tKSlBWVoaYmJhmV5SIiNoWh+6AXnzxRTz66KMICwvD+fPnsWDBAri5ueHJJ5+E0WjE9OnTMXfuXPj7+8PX1xepqamIiYnhG3BERKTiUAD617/+hSeffBKXL19GYGAghg8fjgMHDiAwMBAA8Pbbb6Ndu3ZITExETU0NEhISsGbNmhapOBERuTbOiOriWuvyaXlWr2Uw6P79+2Xp2tpaVRktA1H1YOvYyvopaTlHvfqJtPR9KAeIKg0bNkyV5+np2dQqNUrLQFTlsW3VT8neOQL69BNpwYGojuGMqERE5JQYgIiISAgGICIiEoIBiIiIhGAAIiIiIRiAiIhICAYgIiISggGIiIiEaNbHSOnuoWUAppYBg5MmTZKlL1682OQ6NVdVVZUqz955ajlHvQaiajmWvfp+/PHHqrygoKAm16m5lNO16HGOQOsNRCV98Q6IiIiEYAAiIiIhGICIiEiIu7oP6MyZM6q80NBQh/ejfEZta4IvIiJHKCdQ1NJfplRWVqbK69mzZ5PrpDfeARERkRAMQEREJAQDEBERCcEAREREQtzVLyE4U2ecs9NrwKCtwZ+iKAdFAvbPU8s56kWPwb+2zlEk5fXXa4BzW6R8maktvtzEOyAiIhKCAYiIiIRgACIiIiHu6j6gtkDLhy9b60ONen2M9NixY7K00WhUlenRo0ej6ZbibB8jbS1nz55tNA2or5stTfkYaWvR67qRdrwDIiIiIRiAiIhICAYgIiISwiBJkiS6ErezWCw2n/lT09nrAyooKLC7j7i4OLtlWvMZur0+oNjYWNU2S5YskaVt9T8p+6iUnK2fwN611TIh3SuvvKIqs2/fPllaSx9QS9HSh2nvN+xsv9+7hdlshq+v7x3X8w6IiIiEYAAiIiIhGICIiEgIBiAiIhKCA1GdiIeHhyytZXbW06dP2y1jr3NVS+erXh20ypcF3NzcVGUqKipk6WvXrqnK2OsUr6urU22jPAdbLyHocZ4RERF2y2i5blo05doqX0KwVUb5EoIWHTp0kKWDg4NVZerr62VpLS8z6PH71Ov3q+XaKmchtfVbpF/xDoiIiIRgACIiIiEcDkA//vgjnnrqKQQEBMDHxwcDBgzA4cOHreslScL8+fPRtWtX+Pj4ID4+HidPntS10kRE5Poc6gO6evUqYmNjERcXhy+++AKBgYE4efIkOnfubC3z2muv4d1338X69esRHh6OjIwMJCQk4MSJE/D29tb9BJzBkCFDGk0DQPv27WXpkJCQFq3T7U6dOtXoei2D9FJSUuyW6dWrl+Y6Ndfly5dl6atXr8rStvpYlOfZqVMnVRl759ma52jvugHAqlWrGl1vb2AtAIwZM0aVp2yH2/8fB4CAgAC7+9WLlnaw9xt2tt/v+fPnZenr16+ryhw8eLDRdFvgUAB69dVX0b17d2RnZ1vzwsPDrf8tSRJWrFiBV155BePGjQMAbNiwASaTCVu2bMHkyZN1qjYREbk6hx7Bff755xg8eDCeeOIJBAUFYdCgQfjggw+s60tLS1FRUYH4+HhrntFoRHR0NIqKimzus6amBhaLRbYQEVHb51AAOnPmDLKystC7d298+eWXmDVrFl544QWsX78ewL9fnzWZTLLtTCaT6tXaWzIzM2E0Gq1L9+7dm3IeRETkYhwKQA0NDbj//vuxbNkyDBo0CM899xyeffZZrF27tskVSE9Ph9lsti7l5eVN3hcREbkOh/qAunbtin79+sny+vbti08//RTAvweeVVZWomvXrtYylZWVuO+++2zu08vLC15eXo5Uw+no1Vmo7BRPSkqyu429Tmi96HWcqVOnytK2vpS7bds2WbqlvrxcXV2tytPjPLV0eLfWddPiiy++0GU/ykHGjzzyiKqM8hH7hg0bdDm2PXq1t5Zre+uJ0C22fmf0K4fugGJjY1FSUiLL++GHHxAWFgbg1xcSgoODkZ+fb11vsVhQXFyMmJgYHapLRERthUN3QHPmzMGwYcOwbNkyTJw4EQcPHsT777+P999/HwBgMBiQlpaGJUuWoHfv3tbXsENCQjB+/PiWqD8REbkohwLQAw88gM2bNyM9PR1/+ctfEB4ejhUrVmDKlCnWMi+99BKuXbuG5557DlVVVRg+fDh27NjRZscAERFR03BGVB0sWLBAll64cKGqzNatW2Xp1rwjtHeJDQZDs/ehdT96sdfmyvYG1G1u62Ov586da/S4rXmOrdXmW7ZsUeXdGsd3i7J9Fy1a1OzjaqVHOzjb71fZ5sr2BsS2uV44IyoRETklBiAiIhKCAYiIiITghHStRPmM19YzaeUEaVo+JKmFXpNx2aPlObvynGxNCtcUynM8duyYLvtV0nKOWj7uqkVrXTdbbWWvH3bkyJFNOpZyMryPP/64SftxlJa2bOo5KSnPSXnO9G+8AyIiIiEYgIiISAgGICIiEoIBiIiIhOBLCE5E2VlZUFBgdxstnavONIBNS6fz8ePHZelLly6pyijPW3mOtrZpLa523Wxdk9u/5wgAI0aMkKWVA4EBoEuXLrJ0ZGSkDrXTh5a2tHVOSnq9qEC/4h0QEREJwQBERERCMAAREZEQDEBERCQEAxAREQnBAEREREIwABERkRAMQEREJAQHoro4PQbYtdZXl7XKy8uTpfft26cqY29g5N69e1XbKAe4iqTXwEg9rp2tr64PHz680ePYqn9sbKwsvWTJkmbXTS9a2lLLNeFAVH3xDoiIiIRgACIiIiEYgIiISAgGICIiEoIBiIiIhGAAIiIiIRiAiIhICAYgIiISggNRXZxeA+ycyeTJk2Xp+Ph4VRlnnhFVC2e6bi01I6oz0WvgL+mLd0BERCQEAxAREQnBAEREREKwD0gH586dk6VtfSBS+Xw8MjKyJasko8fHSLWUac0PNSqPpUxr+RjpzZs3VWXsnWdrnqMzfYxUJD3O0dn6d5S/RVt9lsq/K20R74CIiEgIBiAiIhLCoQDUo0cPGAwG1ZKcnAzg10caycnJCAgIQMeOHZGYmIjKysoWqTgREbk2gyRJktbCP/30E+rr663p48eP46GHHkJBQQFGjhyJWbNmYfv27cjJyYHRaERKSgratWtnc0KxO7FYLDAajY6dhQvQ0gcUFBQkS9san6EUFxdnt0xrTTinpX9EeU7KcwaAV155RZa29fs5e/Zso+mWouUcCwoK7JZxpuumRY8ePRpNA9ompLt48aIsbav/ScmZfr9arq3ynJTnDGjrA2oLzGYzfH1977jeoZcQAgMDZenly5cjIiICI0aMgNlsxrp167Bx40aMGjUKAJCdnY2+ffviwIEDGDp0aBOqT0REbVWT+4Bqa2vx97//Hc888wwMBgOOHDmCuro62aj1Pn36IDQ0FEVFRXfcT01NDSwWi2whIqK2r8kBaMuWLaiqqsK0adMAABUVFfD09ISfn5+snMlkQkVFxR33k5mZCaPRaF26d+/e1CoREZELaXIAWrduHcaMGYOQkJBmVSA9PR1ms9m6lJeXN2t/RETkGhx6CeGWc+fOoWfPnvjss88wbtw4AMDu3bsxevRoXL16VXYXFBYWhrS0NMyZM0fTvlvzJQRbHalubm6y9NixY1VlevbsKUsXFxfL0gcPHlRtc/36dVn6woULWqvZbBEREY2uT01NtbuPlStX2i1z+vRpzXVqLn9/f1m6c+fOsrTyGgHqa2nrce/69esbPW5rnqMW9q5tUlKSKk/ZKbx9+3ZVmTNnzsjSV69elaWvXLmitYrNZu8cAfu/YWf7/Xbt2lWWbt++varMkCFDZOno6GhZWnmNAPW1vP2lsVta64UdwP5LCE26A8rOzkZQUJDsf+ioqCh4eHjIvqJbUlKCsrIyxMTENOUwRETUhjn8KZ6GhgZkZ2cjKSkJ7u7/3txoNGL69OmYO3cu/P394evri9TUVMTExPANOCIiUnE4AO3atQtlZWV45plnVOvefvtttGvXDomJiaipqUFCQgLWrFmjS0WJiKhtaVIfUEvSsw9Ij+fjelE+r7X13F35vFbLs1o9no/rRctz9rKyMlm6rq5OVUaP5+MtpTX7Epzp2urVz+nh4SFLh4aG2j22M/1+tVxbZd+ysl8ZUPdH2uqz1IPofs4W6QMiIiJqLgYgIiISggGIiIiEYAAiIiIh2vSMqPY+62Pra721tbUtUhctX8O+ceOGLK3lJQQtny5KS0uzW0YPWr4mrOVLFwEBAbK0su0AqL4v+NFHH9ndrx60nKNenbjOdG21fA1b+UVnWy8hGAwGWdqZzlGva2symWRpHx8fVZmNGzfK0i31NWxPT09V3rBhwxrdpjUH5PIOiIiIhGAAIiIiIRiAiIhIiDbdB2Rv9kLl4DpA3Q+jnJ0TUM/QeWtKiltsDXAtKSmRpfPy8hqtm1ZaZmi0N6Okltk5tRxn0aJFdstooeybu/XB29vZmxF17969qm0yMjJkaVuzsdqbhVavc9SLvetia8ZR5QydixcvVpUZPny4LK2lD2jr1q2y9Pjx4xutm1Z6zDCrZR9a+oCUfVhNpfyN33vvvaoyygGjOTk5srRyBlpb+7XV/2Rv0LZe56gF74CIiEgIBiAiIhKCAYiIiIRo031A9mj5gKWtvgQle/0Rotl7tq3le7T2+pFam70+CVvnrOzPszXma//+/c2tmlOxNe7K1tgQV+dk31TWxYIFC2Tp7OxsQTVpObwDIiIiIRiAiIhICAYgIiISggGIiIiEuKtfQtCLsoPe1oyDyoFmyg5GQD1A0NYgQlG0DMC0dU5KynNSnnNrqqqqUuXZO08t56jXCxtajmWvvrYG1toagNta/Pz8ZGk9zhHQNoi0tcydO1eWttXeyoHotgamKwe0Kwe8twW8AyIiIiEYgIiISAgGICIiEoIBiIiIhGAAIiIiIRiAiIhICAYgIiISggGIiIiE4EBUHSgHwdmaLVI5I6qtwXXK2VidiV4DBm0N/hRFOSgSsH+erTkjqh6Df22do0jK66/XAGdn8tZbb8nStmYlnTx5cqNpwPbfkbaGd0BERCQEAxAREQnBAEREREIwABERkRAMQEREJAQDEBERCeFQAKqvr0dGRgbCw8Ph4+ODiIgILF68GJIkWctIkoT58+eja9eu8PHxQXx8PE6ePKl7xYmIyLU5NA7o1VdfRVZWFtavX4/+/fvj8OHDePrpp2E0GvHCCy8AAF577TW8++67WL9+PcLDw5GRkYGEhAScOHEC3t7eLXISonFCul9xQrpfcUK6O+OEdL/ihHS/cigA7d+/H+PGjcPYsWMB/DpQ6qOPPsLBgwcB/Hr3s2LFCrzyyisYN24cAGDDhg0wmUzYsmWLzcFWRER0d3LoEdywYcOQn5+PH374AQDw7bffYu/evRgzZgwAoLS0FBUVFYiPj7duYzQaER0djaKiIpv7rKmpgcVikS1ERNT2OXQHNG/ePFgsFvTp0wdubm6or6/H0qVLMWXKFABARUUFAMBkMsm2M5lM1nVKmZmZrfp5EyIicg4O3QF98sknyM3NxcaNG/H1119j/fr1eOONN2z2eWiVnp4Os9lsXcrLy5u8LyIich0O3QH96U9/wrx586x9OQMGDMC5c+eQmZmJpKQkBAcHAwAqKyvRtWtX63aVlZW47777bO7Ty8sLXl5eTax+4wwGQ6Prz507p8oLDQ1tkbrs27dPln7vvfd02a+9cwQge0uxJcXFxemyn/Hjx9sto+y8XrhwoS7Htkevc9TrWK3V+a5s36Y+tVC+dOJM56jl/yUttLw8FBkZKUt36dJFl2MrlZWVqfLCwsJa5FhN4dAd0PXr19GunXwTNzc3NDQ0AADCw8MRHByM/Px863qLxYLi4mLExMToUF0iImorHLoDevTRR7F06VKEhoaif//++Oabb/DWW2/hmWeeAfDrvyDS0tKwZMkS9O7d2/oadkhIiKZ/1RIR0d3DoQC0cuVKZGRk4Pnnn8fFixcREhKCGTNmYP78+dYyL730Eq5du4bnnnsOVVVVGD58OHbs2NFmxwAREVHTGKTW6iDQyGKxwGg0iq6GlfJZLaB+Xnv27NlG085OyzN2vQZXthZbz9SV19LWBIDFxcUtVicRoqOjVXnKCdKOHz+uKnPp0qUWqxPdPcxmM3x9fe+4nt+CIyIiIRiAiIhICAYgIiISggGIiIiE4EsIRETUIvgSAhEROSUGICIiEsLpApCTPREkIqImsvf33OkCUHV1tegqEBGRDuz9PXe6lxAaGhpw/vx5dOrUCdXV1ejevTvKy8sb7ciiprFYLGzfFsT2bVls35bVnPaVJAnV1dUICQlRfcD6dg59C641tGvXDt26dQPw78+j+/r68gfWgti+LYvt27LYvi2rqe2r5W1mp3sER0REdwcGICIiEsKpA5CXlxcWLFjQYjOm3u3Yvi2L7duy2L4tqzXa1+leQiAioruDU98BERFR28UAREREQjAAERGREAxAREQkBAMQEREJ4bQBaPXq1ejRowe8vb0RHR2NgwcPiq6SS8rMzMQDDzyATp06ISgoCOPHj0dJSYmszM2bN5GcnIyAgAB07NgRiYmJqKysFFRj17V8+XIYDAakpaVZ89i2zffjjz/iqaeeQkBAAHx8fDBgwAAcPnzYul6SJMyfPx9du3aFj48P4uPjcfLkSYE1dh319fXIyMhAeHg4fHx8EBERgcWLF8s+Itqi7Ss5oby8PMnT01P629/+Jn333XfSs88+K/n5+UmVlZWiq+ZyEhISpOzsbOn48ePS0aNHpd///vdSaGio9PPPP1vLzJw5U+revbuUn58vHT58WBo6dKg0bNgwgbV2PQcPHpR69OghDRw4UJo9e7Y1n23bPFeuXJHCwsKkadOmScXFxdKZM2ekL7/8Ujp16pS1zPLlyyWj0Sht2bJF+vbbb6U//OEPUnh4uHTjxg2BNXcNS5culQICAqRt27ZJpaWl0qZNm6SOHTtK77zzjrVMS7avUwagIUOGSMnJydZ0fX29FBISImVmZgqsVdtw8eJFCYC0Z88eSZIkqaqqSvLw8JA2bdpkLfN///d/EgCpqKhIVDVdSnV1tdS7d29p586d0ogRI6wBiG3bfC+//LI0fPjwO65vaGiQgoODpddff92aV1VVJXl5eUkfffRRa1TRpY0dO1Z65plnZHkTJkyQpkyZIklSy7ev0z2Cq62txZEjRxAfH2/Na9euHeLj41FUVCSwZm2D2WwGAPj7+wMAjhw5grq6Oll79+nTB6GhoWxvjZKTkzF27FhZGwJsWz18/vnnGDx4MJ544gkEBQVh0KBB+OCDD6zrS0tLUVFRIWtjo9GI6OhotrEGw4YNQ35+Pn744QcAwLfffou9e/dizJgxAFq+fZ3ua9iXLl1CfX09TCaTLN9kMuH7778XVKu2oaGhAWlpaYiNjUVkZCQAoKKiAp6envDz85OVNZlMqKioEFBL15KXl4evv/4ahw4dUq1j2zbfmTNnkJWVhblz5+LPf/4zDh06hBdeeAGenp5ISkqytqOtvxdsY/vmzZsHi8WCPn36wM3NDfX19Vi6dCmmTJkCAC3evk4XgKjlJCcn4/jx49i7d6/oqrQJ5eXlmD17Nnbu3Alvb2/R1WmTGhoaMHjwYCxbtgwAMGjQIBw/fhxr165FUlKS4Nq5vk8++QS5ubnYuHEj+vfvj6NHjyItLQ0hISGt0r5O9wiuS5cucHNzU70pVFlZieDgYEG1cn0pKSnYtm0bCgoKrPMtAUBwcDBqa2tRVVUlK8/2tu/IkSO4ePEi7r//fri7u8Pd3R179uzBu+++C3d3d5hMJrZtM3Xt2hX9+vWT5fXt2xdlZWUAYG1H/r1omj/96U+YN28eJk+ejAEDBuCPf/wj5syZg8zMTAAt375OF4A8PT0RFRWF/Px8a15DQwPy8/MRExMjsGauSZIkpKSkYPPmzdi9ezfCw8Nl66OiouDh4SFr75KSEpSVlbG97Rg9ejSOHTuGo0ePWpfBgwdjypQp1v9m2zZPbGysatjADz/8gLCwMABAeHg4goODZW1ssVhQXFzMNtbg+vXrqhlL3dzc0NDQAKAV2rfZrzG0gLy8PMnLy0vKycmRTpw4IT333HOSn5+fVFFRIbpqLmfWrFmS0WiUCgsLpQsXLliX69evW8vMnDlTCg0NlXbv3i0dPnxYiomJkWJiYgTW2nXd/hacJLFtm+vgwYOSu7u7tHTpUunkyZNSbm6u1L59e+nvf/+7tczy5cslPz8/aevWrdI///lPady4cXwNW6OkpCTpnnvusb6G/dlnn0ldunSRXnrpJWuZlmxfpwxAkiRJK1eulEJDQyVPT09pyJAh0oEDB0RXySUBsLlkZ2dby9y4cUN6/vnnpc6dO0vt27eXHnvsMenChQviKu3ClAGIbdt8//M//yNFRkZKXl5eUp8+faT3339ftr6hoUHKyMiQTCaT5OXlJY0ePVoqKSkRVFvXYrFYpNmzZ0uhoaGSt7e31LNnT+m//uu/pJqaGmuZlmxfzgdERERCOF0fEBER3R0YgIiISAgGICIiEoIBiIiIhGAAIiIiIRiAiIhICAYgIiISggGIiIiEYAAiIiIhGICIiEgIBiAiIhLi/wHIBoNqQNVMXAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "state, _ = env.reset()\n",
    "processed_frame = preprocess_frame(state, augment=False)\n",
    "plt.imshow(processed_frame, cmap='gray')\n",
    "plt.title(\"Augmented Frame\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:48:29.413248Z",
     "start_time": "2024-12-10T14:48:29.252829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 21, 21, 32)        2080      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 21, 21, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 10, 10, 64)        8256      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 10, 10, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 10, 10, 64)        4160      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 10, 10, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6400)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                409664    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 425,190\n",
      "Trainable params: 424,870\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the DQN model\n",
    "def create_dqn(action_space):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (4, 4), strides=4, activation='relu', input_shape=(84, 84, 4)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (2, 2), strides=2, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (1, 1), strides=1, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(action_space, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Initialize the model\n",
    "dqn_model = create_dqn(env.action_space.n)\n",
    "dqn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:50:20.065494Z",
     "start_time": "2024-12-10T14:50:20.049832Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(next_states),\n",
    "            np.array(dones)\n",
    "        )\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "replay_buffer = ReplayBuffer(capacity=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:50:21.460118Z",
     "start_time": "2024-12-10T14:50:21.444472Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def select_action(model, state, epsilon, action_space):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_space - 1)  # Random action\n",
    "    else:\n",
    "        q_values = model.predict(state[None, ...], verbose=0)\n",
    "        return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:50:26.608486Z",
     "start_time": "2024-12-10T14:50:26.592747Z"
    }
   },
   "outputs": [],
   "source": [
    "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "def record_gameplay(env, model, video_path=\"trained_agent_gameplay.mp4\"):\n",
    "    # Initialize video recorder\n",
    "    recorder = VideoRecorder(env, video_path)\n",
    "\n",
    "    # Initialize frame stack\n",
    "    frame_stack = deque(maxlen=4)\n",
    "\n",
    "    # Reset the environment\n",
    "    state, info = env.reset()\n",
    "    processed_frame = preprocess_frame(state)\n",
    "\n",
    "    # Stack the same frame 4 times initially\n",
    "    for _ in range(4):\n",
    "        frame_stack.append(processed_frame)\n",
    "\n",
    "    state = np.stack(frame_stack, axis=-1)  # Create the 4-channel input\n",
    "\n",
    "    for _ in range(500):  # Limit steps in the episode\n",
    "        # Predict action based on the current state\n",
    "        action = np.argmax(model.predict(state[None, ...], verbose=0))\n",
    "\n",
    "        # Take a step in the environment\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Record the frame\n",
    "        recorder.capture_frame()\n",
    "\n",
    "        # Preprocess the next frame and update the stack\n",
    "        processed_frame = preprocess_frame(next_state)\n",
    "        frame_stack.append(processed_frame)\n",
    "        state = np.stack(frame_stack, axis=-1)  # Update input state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Close the recorder and save the video\n",
    "    recorder.close()\n",
    "    recorder.enabled = False\n",
    "    print(f\"Gameplay recorded at recordings/{video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T16:04:09.378489Z",
     "start_time": "2024-12-10T15:08:31.599830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Reward: -340.00, Survival Time: 328, Epsilon: 0.980, Life Count: 0\n",
      "Episode 2/100, Reward: -380.00, Survival Time: 244, Epsilon: 0.960, Life Count: 0\n",
      "Episode 3/100, Reward: -355.00, Survival Time: 293, Epsilon: 0.941, Life Count: 0\n",
      "Episode 4/100, Reward: -350.00, Survival Time: 315, Epsilon: 0.922, Life Count: 0\n",
      "Episode 5/100, Reward: -405.00, Survival Time: 284, Epsilon: 0.904, Life Count: 0\n",
      "Episode 6/100, Reward: -110.00, Survival Time: 477, Epsilon: 0.886, Life Count: 0\n",
      "Episode 7/100, Reward: -125.00, Survival Time: 431, Epsilon: 0.868, Life Count: 0\n",
      "Episode 8/100, Reward: -85.00, Survival Time: 427, Epsilon: 0.851, Life Count: 0\n",
      "Episode 9/100, Reward: -355.00, Survival Time: 299, Epsilon: 0.834, Life Count: 0\n",
      "Episode 10/100, Reward: -350.00, Survival Time: 305, Epsilon: 0.817, Life Count: 0\n",
      "Episode 11/100, Reward: -200.00, Survival Time: 343, Epsilon: 0.801, Life Count: 0\n",
      "Episode 12/100, Reward: -150.00, Survival Time: 363, Epsilon: 0.785, Life Count: 0\n",
      "Episode 13/100, Reward: 125.00, Survival Time: 445, Epsilon: 0.769, Life Count: 0\n",
      "Episode 14/100, Reward: 215.00, Survival Time: 434, Epsilon: 0.754, Life Count: 0\n",
      "Episode 15/100, Reward: -285.00, Survival Time: 334, Epsilon: 0.739, Life Count: 0\n",
      "Episode 16/100, Reward: 15.00, Survival Time: 346, Epsilon: 0.724, Life Count: 0\n",
      "Episode 17/100, Reward: 175.00, Survival Time: 374, Epsilon: 0.709, Life Count: 0\n",
      "Episode 18/100, Reward: -375.00, Survival Time: 302, Epsilon: 0.695, Life Count: 0\n",
      "Episode 19/100, Reward: -355.00, Survival Time: 278, Epsilon: 0.681, Life Count: 0\n",
      "Episode 20/100, Reward: 350.00, Survival Time: 515, Epsilon: 0.668, Life Count: 0\n",
      "Episode 21/100, Reward: -335.00, Survival Time: 333, Epsilon: 0.654, Life Count: 0\n",
      "Episode 22/100, Reward: -275.00, Survival Time: 319, Epsilon: 0.641, Life Count: 0\n",
      "Episode 23/100, Reward: -230.00, Survival Time: 349, Epsilon: 0.628, Life Count: 0\n",
      "Episode 24/100, Reward: -380.00, Survival Time: 318, Epsilon: 0.616, Life Count: 0\n",
      "Episode 25/100, Reward: -335.00, Survival Time: 288, Epsilon: 0.603, Life Count: 0\n",
      "Episode 26/100, Reward: -180.00, Survival Time: 369, Epsilon: 0.591, Life Count: 0\n",
      "Episode 27/100, Reward: -285.00, Survival Time: 473, Epsilon: 0.580, Life Count: 0\n",
      "Episode 28/100, Reward: -205.00, Survival Time: 427, Epsilon: 0.568, Life Count: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 91\u001b[0m\n\u001b[0;32m     88\u001b[0m target_model \u001b[38;5;241m=\u001b[39m create_dqn(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[0;32m     89\u001b[0m target_model\u001b[38;5;241m.\u001b[39mset_weights(dqn_model\u001b[38;5;241m.\u001b[39mget_weights())  \u001b[38;5;66;03m# Synchronize weights\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m rewards_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdqn_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust for quick testing\u001b[39;49;00m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.98\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# TODO: Test varying this\u001b[39;49;00m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m    104\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 58\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[1;34m(env, model, target_model, replay_buffer, episodes, batch_size, gamma, epsilon, epsilon_min, epsilon_decay, update_target, verbose)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Double DQN: Use the model to select actions, but the target model to evaluate Q-values\u001b[39;00m\n\u001b[0;32m     57\u001b[0m next_q_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(model\u001b[38;5;241m.\u001b[39mpredict(next_states, verbose\u001b[38;5;241m=\u001b[39mverbose), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# TODO: Print out q values\u001b[39;00m\n\u001b[0;32m     59\u001b[0m target_q_values \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m next_q_values[np\u001b[38;5;241m.\u001b[39marange(batch_size), next_q_actions] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n",
      "File \u001b[1;32mc:\\Users\\tadhg\\.conda\\envs\\tf-env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\tadhg\\.conda\\envs\\tf-env\\lib\\site-packages\\keras\\engine\\training.py:2253\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m   2252\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 2253\u001b[0m     tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   2255\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\tadhg\\.conda\\envs\\tf-env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\tadhg\\.conda\\envs\\tf-env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\tadhg\\.conda\\envs\\tf-env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tadhg\\.conda\\envs\\tf-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tadhg\\.conda\\envs\\tf-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\tadhg\\.conda\\envs\\tf-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\tadhg\\.conda\\envs\\tf-env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_dqn(env, model, target_model, replay_buffer, episodes=500, batch_size=32, gamma=0.99, epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.995, update_target=10, verbose=0):\n",
    "    \"\"\"\n",
    "    Train the DQN model using survival-based rewards.\n",
    "    \"\"\"\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    loss_fn = tf.keras.losses.Huber()\n",
    "    rewards_history = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = reset_env_with_stack(env)\n",
    "        total_reward = 0\n",
    "        episode_loss = 0\n",
    "        last_lives = 4\n",
    "        num_of_NOOPs = 0\n",
    "\n",
    "        for step in range(10_000):  # Limit steps per episode\n",
    "            action = select_action(model, state, epsilon, env.action_space.n)\n",
    "            next_state, reward, done, truncated, info = step_env_with_stack(env, action)\n",
    "\n",
    "            # TODO: Test reward function\n",
    "            # (gymnasium.wrappers.TransformReward)[https:#gymnasium.farama.org/v0.27.0/api/wrappers/reward_wrappers/]\n",
    "            # # Calculate adjusted reward\n",
    "            # base_reward = 1  # Reward for surviving\n",
    "            # penalty = -120 / survival_time if done else 0  # Penalize more for short survival\n",
    "            # bonus = 15 if reward > 0  else 0\n",
    "            # adjusted_reward = base_reward + reward + penalty + bonus\n",
    "\n",
    "            # if survival_time == 350:  # Encourage longer survival\n",
    "            #     adjusted_reward += 25\n",
    "\n",
    "            adjusted_reward = reward\n",
    "            if info.get(\"lives\") < last_lives:\n",
    "                last_lives = info.get(\"lives\")\n",
    "                adjusted_reward -= 100\n",
    "\n",
    "            if action == 0: # TODO: check if fire should be included here\n",
    "                num_of_NOOPs += 1\n",
    "                if num_of_NOOPs >= 3:\n",
    "                    adjusted_reward -= 5\n",
    "            else:\n",
    "                num_of_NOOPs = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Store adjusted reward in replay buffer\n",
    "            replay_buffer.add((state, action, adjusted_reward, next_state, done))\n",
    "            state = next_state\n",
    "            total_reward += adjusted_reward\n",
    "\n",
    "            if replay_buffer.size() > batch_size:\n",
    "                # Sample from the replay buffer\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size=batch_size)\n",
    "\n",
    "                # Double DQN: Use the model to select actions, but the target model to evaluate Q-values\n",
    "                next_q_actions = np.argmax(model.predict(next_states, verbose=verbose), axis=1)\n",
    "                next_q_values = target_model.predict(next_states, verbose=verbose) # TODO: Print out q values\n",
    "                target_q_values = rewards + gamma * next_q_values[np.arange(batch_size), next_q_actions] * (1 - dones)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    q_values = model(states)\n",
    "                    q_values = tf.reduce_sum(q_values * tf.one_hot(actions, env.action_space.n), axis=1)\n",
    "                    loss = loss_fn(target_q_values, q_values)\n",
    "                    episode_loss += loss.numpy()\n",
    "\n",
    "                grads = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if episode == 40:\n",
    "            epsilon = min(0.3, epsilon * epsilon_decay)  # Drop epsilon to 0.3 after episode 40\n",
    "        else:\n",
    "            # Update epsilon for exploration-exploitation tradeoff\n",
    "            epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        # Update target model weights periodically\n",
    "        if episode % update_target == 0:\n",
    "            target_model.set_weights(model.get_weights())\n",
    "\n",
    "        rewards_history.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Reward: {total_reward:.2f}, Survival Time: {step}, Epsilon: {epsilon:.3f}, Life Count: {last_lives}\")\n",
    "\n",
    "    return rewards_history\n",
    "\n",
    "target_model = create_dqn(env.action_space.n)\n",
    "target_model.set_weights(dqn_model.get_weights())  # Synchronize weights\n",
    "\n",
    "rewards_history = train_dqn(\n",
    "    env=env,\n",
    "    model=dqn_model,\n",
    "    target_model=target_model,\n",
    "    replay_buffer=replay_buffer,\n",
    "    episodes=100,  # Adjust for quick testing\n",
    "    batch_size=32,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_min=0.1,\n",
    "    epsilon_decay=0.98,\n",
    "    update_target=5, # TODO: Test varying this\n",
    "    verbose=0\n",
    ")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_gameplay(env, dqn_model, video_path=\"trained_agent_gameplay.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's weights\n",
    "dqn_model.save_weights(\"models/dqn_model_weights.h5\")\n",
    "print(\"Model weights saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T06:13:34.891012Z",
     "start_time": "2024-12-10T06:13:34.805497Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "dqn_model.save(\"models/dqn_model.h5\")\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T13:44:23.063705Z",
     "start_time": "2024-12-10T13:43:28.665797Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, model, episodes=10):\n",
    "    total_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state, _ = reset_env_with_stack(env)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(model.predict(state[None, ...], verbose=0))\n",
    "            next_state, reward, done, truncated, _ = step_env_with_stack(env, action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}, Reward: {total_reward}\")\n",
    "\n",
    "    print(f\"Average Reward: {np.mean(total_rewards)}\")\n",
    "\n",
    "evaluate_agent(env, dqn_model, episodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T13:47:19.945882Z",
     "start_time": "2024-12-10T13:47:19.882484Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_rewards(rewards):\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Training Rewards')\n",
    "    plt.show()\n",
    "\n",
    "# Example\n",
    "plot_rewards(rewards_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T13:47:26.137907Z",
     "start_time": "2024-12-10T13:47:26.090493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate key statistics\n",
    "average_reward = np.mean(rewards_history)\n",
    "max_reward = np.max(rewards_history)\n",
    "\n",
    "# Plot the rewards with highlights\n",
    "plt.plot(rewards_history, label=\"Raw Rewards\")\n",
    "plt.axhline(average_reward, color='r', linestyle='--', label=f\"Average Reward: {average_reward:.2f}\")\n",
    "plt.axhline(max_reward, color='g', linestyle='--', label=f\"Max Reward: {max_reward}\")\n",
    "plt.title(\"Training Rewards with Statistics\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T06:20:48.462414Z",
     "start_time": "2024-12-10T06:20:48.186683Z"
    }
   },
   "outputs": [],
   "source": [
    "window = 10  # Adjust the window size as needed\n",
    "smoothed_rewards = np.convolve(rewards_history, np.ones(window)/window, mode='valid')\n",
    "\n",
    "# Plot smoothed rewards\n",
    "plt.plot(smoothed_rewards)\n",
    "plt.title(\"Smoothed Training Rewards\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T06:21:27.632894Z",
     "start_time": "2024-12-10T06:21:27.334487Z"
    }
   },
   "outputs": [],
   "source": [
    "# Segment rewards into blocks\n",
    "block_size = 10\n",
    "block_means = [np.mean(rewards_history[i:i+block_size]) for i in range(0, len(rewards_history), block_size)]\n",
    "\n",
    "# Plot block means\n",
    "plt.plot(block_means)\n",
    "plt.title(\"Mean Reward Per Block of Episodes\")\n",
    "plt.xlabel(\"Blocks (10 episodes each)\")\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T06:21:04.444648Z",
     "start_time": "2024-12-10T06:21:04.047819Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate cumulative rewards\n",
    "cumulative_rewards = np.cumsum(rewards_history)\n",
    "\n",
    "# Plot cumulative rewards\n",
    "plt.plot(cumulative_rewards)\n",
    "plt.title(\"Cumulative Training Rewards\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
