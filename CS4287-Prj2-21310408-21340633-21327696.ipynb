{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Project Part 3\n",
    "\n",
    "Code runs through until completion\n",
    "\n",
    "## Contributors: \n",
    "- Tadhg Ryan (21310408)\n",
    "- Craig Phayer (21340633)\n",
    "- Thomas McCarty (21327696)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs\n",
    "\n",
    "# %pip install --upgrade pip\n",
    "# %pip install -q -U gymnasium swig\n",
    "# %pip install gymnasium[atari]==0.29.1\n",
    "# %pip install gymnasium[classic_control,box2d,accept-rom-license,ActionWrapper]==0.29.1\n",
    "# %pip install tensorflow==2.10\n",
    "# %pip install \"numpy<2\"\n",
    "# %pip install matplotlib\n",
    "# %pip install opencv-python\n",
    "# %pip install moviepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import cv2\n",
    "from collections import deque\n",
    "import os\n",
    "from tensorflow.keras import layers, models\n",
    "import random\n",
    "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "from gymnasium.core import Wrapper\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:38:14.554959Z",
     "start_time": "2024-12-10T14:38:11.859470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show Hardware\n",
    "\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(\"GPUs available:\")\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(\"Name:\", details.get('device_name', 'Unknown GPU'))\n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:40:58.327484Z",
     "start_time": "2024-12-10T14:40:58.264785Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make Env\n",
    "\n",
    "env = gym.make(\"ALE/Qbert-v5\", obs_type=\"ram\", frameskip=4, render_mode=\"rgb_array\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Observation space:\", env.observation_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:41:00.314343Z",
     "start_time": "2024-12-10T14:41:00.298417Z"
    }
   },
   "outputs": [],
   "source": [
    "# Env Functions\n",
    "\n",
    "frame_stack = deque(maxlen=4)\n",
    "\n",
    "def reset_env_with_stack(env):\n",
    "    state, info = env.reset(seed=42)\n",
    "    for _ in range(4):\n",
    "        frame_stack.append(state)\n",
    "    return np.stack(frame_stack, axis=-1), info\n",
    "\n",
    "def step_env_with_stack(env, action):\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    frame_stack.append(state)\n",
    "    return np.stack(frame_stack, axis=-1), reward, done, truncated, info\n",
    "\n",
    "state, _ = env.reset(seed=42)\n",
    "print(\"Processed frame shape:\", state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:48:29.413248Z",
     "start_time": "2024-12-10T14:48:29.252829Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Model\n",
    "\n",
    "def create_dqn(action_space):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(128, 4)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(action_space, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Initialize the model\n",
    "dqn_model = create_dqn(env.action_space.n)\n",
    "dqn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:50:20.065494Z",
     "start_time": "2024-12-10T14:50:20.049832Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, beta=0.4, epsilon=1e-6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.buffer = []\n",
    "        \n",
    "        self.priorities = []\n",
    "        \n",
    "        self.size = 0\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if self.size < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "            self.priorities.append(max(self.priorities, default=1.0))\n",
    "            self.size += 1\n",
    "        else:\n",
    "            self.buffer[self.idx] = experience\n",
    "            self.priorities[self.idx] = max(self.priorities, default=1.0)\n",
    "        self.idx = (self.idx + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        priorities = np.array(self.priorities) ** self.alpha\n",
    "        sampling_probs = priorities / sum(priorities)\n",
    "\n",
    "        indices = np.random.choice(np.arange(self.size), size=batch_size, p=sampling_probs)\n",
    "        \n",
    "        experiences = [self.buffer[idx] for idx in indices]\n",
    "        weights = (self.size * sampling_probs[indices]) ** -self.beta\n",
    "        weights = weights / weights.max()\n",
    "\n",
    "        return experiences, weights, indices\n",
    "\n",
    "    def update_priorities(self, indices, errors):\n",
    "        for idx, error in zip(indices, errors):\n",
    "            self.priorities[idx] = (abs(error) + self.epsilon) ** self.alpha\n",
    "\n",
    "    def size(self):\n",
    "        return self.size\n",
    "\n",
    "replay_buffer = PrioritizedReplayBuffer(capacity=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:50:21.460118Z",
     "start_time": "2024-12-10T14:50:21.444472Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select Action\n",
    "\n",
    "def select_action(model, state, epsilon, action_space):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_space - 1)\n",
    "    else:\n",
    "        q_values = model.predict(state[None, ...], verbose=0)\n",
    "        return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:50:26.608486Z",
     "start_time": "2024-12-10T14:50:26.592747Z"
    }
   },
   "outputs": [],
   "source": [
    "# Record Gameplay\n",
    "\n",
    "def record_gameplay(env, model, video_path=\"trained_agent_gameplay.mp4\"):\n",
    "    recorder = VideoRecorder(env, video_path)\n",
    "\n",
    "    frame_stack = deque(maxlen=4)\n",
    "\n",
    "    state, info = env.reset(seed=42)\n",
    "    \n",
    "    for _ in range(4):\n",
    "        frame_stack.append(state)\n",
    "\n",
    "    state = np.stack(frame_stack, axis=-1)\n",
    "\n",
    "    for _ in range(500):\n",
    "        action = np.argmax(model.predict(state[None, ...], verbose=0))\n",
    "\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        recorder.capture_frame()\n",
    "\n",
    "        frame_stack.append(next_state)\n",
    "        state = np.stack(frame_stack, axis=-1)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    recorder.close()\n",
    "    recorder.enabled = False\n",
    "    print(f\"Gameplay recorded at recordings/{video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward Function\n",
    "\n",
    "class CustomRewardWrapper(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, truncated, info = self.env.step(action)\n",
    "\n",
    "        reward = self.custom_reward_function(reward, info)\n",
    "\n",
    "        return state, reward, done, truncated, info\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_reward_function(reward, info):\n",
    "        adjusted_reward = reward\n",
    "\n",
    "        if \"lives\" in info and info[\"lives\"] < 4:\n",
    "            adjusted_reward -= 50 * (4 - info[\"lives\"])\n",
    "\n",
    "        if reward > 0:\n",
    "            adjusted_reward += 25\n",
    "\n",
    "        if \"cube_color_changed\" in info:\n",
    "            adjusted_reward += 10 \n",
    "\n",
    "        if \"cube_destination_reached\" in info:\n",
    "            adjusted_reward += 50\n",
    "\n",
    "        if \"cube_reverted\" in info:\n",
    "            adjusted_reward -= 15\n",
    "\n",
    "        if \"level_completed\" in info:\n",
    "            adjusted_reward += 1000\n",
    "            adjusted_reward += max(0, 200 - info.get(\"steps\", 0))\n",
    "\n",
    "        if \"avoided_enemy\" in info:\n",
    "            adjusted_reward += 10\n",
    "\n",
    "        if \"caught_green_ball\" in info:\n",
    "            adjusted_reward += 50\n",
    "\n",
    "        if \"coily_defeated\" in info:\n",
    "            adjusted_reward += 100\n",
    "\n",
    "        if info.get(\"no_op\", False) or info.get(\"fire\", False):\n",
    "            adjusted_reward -= 1\n",
    "\n",
    "        scaling_factor = info.get(\"episode\", 0) / 100\n",
    "        adjusted_reward += scaling_factor * adjusted_reward\n",
    "\n",
    "        return adjusted_reward\n",
    "\n",
    "env = CustomRewardWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T16:04:09.378489Z",
     "start_time": "2024-12-10T15:08:31.599830Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train DQN\n",
    "\n",
    "def soft_decay_interpolation(t, a, b, k=10):\n",
    "    return b - (b - a) / (1.0 + np.exp(k * (t - 0.5)))\n",
    "\n",
    "def train_dqn(env, model, target_model, replay_buffer, episodes=500, batch_size=32, gamma=0.99, epsilon_min=0.1, epsilon_max=1, update_target=10, verbose=0):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    loss_fn = tf.keras.losses.Huber()\n",
    "    rewards_history = []\n",
    "    epsilon = epsilon_max\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = reset_env_with_stack(env)\n",
    "        total_reward = 0\n",
    "        episode_loss = 0\n",
    "        total_lives = 4\n",
    "\n",
    "        epsilon_reward = epsilon * 50\n",
    "\n",
    "        for step in range(10_000):\n",
    "            action = select_action(model, state, epsilon, env.action_space.n)\n",
    "            next_state, reward, done, truncated, info = step_env_with_stack(env, action)\n",
    "\n",
    "            replay_buffer.add((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if step == 0:\n",
    "                reward += epsilon_reward\n",
    "\n",
    "            if replay_buffer.size > batch_size:\n",
    "                experiences, weights, indices = replay_buffer.sample(batch_size=batch_size)\n",
    "\n",
    "                states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "                rewards = np.array(rewards)\n",
    "                dones = np.array(dones)\n",
    "                states = np.array(states)\n",
    "                states = states.reshape(batch_size, 128, 4)\n",
    "                next_states = np.array(next_states)\n",
    "                next_states = next_states.reshape(batch_size, 128, 4)\n",
    "\n",
    "                next_q_actions = np.argmax(model.predict(next_states, verbose=verbose), axis=1)\n",
    "                next_q_values = target_model.predict(next_states, verbose=verbose)\n",
    "                target_q_values = rewards + gamma * next_q_values[np.arange(batch_size), next_q_actions] * (1 - dones)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    q_values = model(states)\n",
    "                    q_values = tf.reduce_sum(q_values * tf.one_hot(actions, env.action_space.n), axis=1)\n",
    "                    loss = loss_fn(target_q_values, q_values)\n",
    "                    weighted_loss = tf.reduce_mean(weights * loss)\n",
    "                    episode_loss += weighted_loss.numpy()\n",
    "\n",
    "                grads = tape.gradient(weighted_loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "                td_errors = np.abs(target_q_values - q_values.numpy())\n",
    "                replay_buffer.update_priorities(indices, td_errors)\n",
    "\n",
    "            if done or truncated or info.get(\"lives\") < total_lives:\n",
    "                break\n",
    "\n",
    "        if episode % update_target == 0:\n",
    "            target_model.set_weights(model.get_weights())\n",
    "\n",
    "        rewards_history.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Reward: {total_reward:.2f}, Survival Time: {step}, Epsilon: {epsilon:.3f}\")\n",
    "        \n",
    "        epsilon = soft_decay_interpolation(episode/(episodes-1), epsilon_max, epsilon_min)\n",
    "\n",
    "    return rewards_history\n",
    "\n",
    "target_model = create_dqn(env.action_space.n)\n",
    "target_model.set_weights(dqn_model.get_weights())\n",
    "\n",
    "rewards_history = train_dqn(\n",
    "    env=env,\n",
    "    model=dqn_model,\n",
    "    target_model=target_model,\n",
    "    replay_buffer=replay_buffer,\n",
    "    episodes=2000,\n",
    "    batch_size=32,\n",
    "    gamma=0.95,\n",
    "    epsilon_min=0.1,\n",
    "    epsilon_max=1.0,\n",
    "    update_target=10,\n",
    "    verbose=0\n",
    ")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record Gameplay\n",
    "\n",
    "record_gameplay(env, dqn_model, video_path=\"trained_agent_gameplay.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T06:13:34.891012Z",
     "start_time": "2024-12-10T06:13:34.805497Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save Model\n",
    "\n",
    "dqn_model.save(\"models/dqn_model.h5\")\n",
    "print(\"Model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T13:44:23.063705Z",
     "start_time": "2024-12-10T13:43:28.665797Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "\n",
    "def evaluate_agent(env, model, episodes=10):\n",
    "    total_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state, _ = reset_env_with_stack(env)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(model.predict(state[None, ...], verbose=0))\n",
    "            next_state, reward, done, truncated, _ = step_env_with_stack(env, action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}, Reward: {total_reward}\")\n",
    "\n",
    "    print(f\"Average Reward: {np.mean(total_rewards)}\")\n",
    "\n",
    "evaluate_agent(env, dqn_model, episodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T13:47:19.945882Z",
     "start_time": "2024-12-10T13:47:19.882484Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot Rewards\n",
    "\n",
    "def plot_rewards(rewards):\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Training Rewards')\n",
    "    plt.show()\n",
    "\n",
    "plot_rewards(rewards_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T13:47:26.137907Z",
     "start_time": "2024-12-10T13:47:26.090493Z"
    }
   },
   "outputs": [],
   "source": [
    "# More Stats\n",
    "\n",
    "average_reward = np.mean(rewards_history)\n",
    "max_reward = np.max(rewards_history)\n",
    "\n",
    "plt.plot(rewards_history, label=\"Raw Rewards\")\n",
    "plt.axhline(average_reward, color='r', linestyle='--', label=f\"Average Reward: {average_reward:.2f}\")\n",
    "plt.axhline(max_reward, color='g', linestyle='--', label=f\"Max Reward: {max_reward}\")\n",
    "plt.title(\"Training Rewards with Statistics\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T06:20:48.462414Z",
     "start_time": "2024-12-10T06:20:48.186683Z"
    }
   },
   "outputs": [],
   "source": [
    "# Even More Stats\n",
    "\n",
    "window = 10\n",
    "smoothed_rewards = np.convolve(rewards_history, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.plot(smoothed_rewards)\n",
    "plt.title(\"Smoothed Training Rewards\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T06:21:27.632894Z",
     "start_time": "2024-12-10T06:21:27.334487Z"
    }
   },
   "outputs": [],
   "source": [
    "# Yet Another Stats Block\n",
    "\n",
    "block_size = 10\n",
    "block_means = [np.mean(rewards_history[i:i+block_size]) for i in range(0, len(rewards_history), block_size)]\n",
    "\n",
    "plt.plot(block_means)\n",
    "plt.title(\"Mean Reward Per Block of Episodes\")\n",
    "plt.xlabel(\"Blocks (10 episodes each)\")\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T06:21:04.444648Z",
     "start_time": "2024-12-10T06:21:04.047819Z"
    }
   },
   "outputs": [],
   "source": [
    "# Actually the Last Stats Block\n",
    "\n",
    "cumulative_rewards = np.cumsum(rewards_history)\n",
    "\n",
    "plt.plot(cumulative_rewards)\n",
    "plt.title(\"Cumulative Training Rewards\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
